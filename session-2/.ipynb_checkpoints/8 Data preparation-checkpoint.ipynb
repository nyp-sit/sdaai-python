{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37puETfgRzzg"
   },
   "source": [
    "# Data Preparation\n",
    "Data preparation is a critical phase in machine learning and it has been said that a good 80% of the effort may be spent from collecting and then preparing data for use. Steps of data cleaning and organization can help to direct the learning towards the intended goal while the lack of them will likely be an unsuccessful model. Data can have discrepancies, errors, outliers and missing attributes of interest and we will see how some of theses issues can be handled in the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EoRP98MpR-qj"
   },
   "source": [
    "## 1 Importing the libraries\n",
    "As per most work, libraries of functions that will be used in the data preparation process need to be imported into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-qiINBQSK2g"
   },
   "outputs": [],
   "source": [
    "# import numpy, matplotlib.pylot and panda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# import arff\n",
    "import requests, io, zipfile\n",
    "from scipy.io import arff\n",
    "\n",
    "# import imputers for handling missing value and encoders\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RopL7tUZSQkT"
   },
   "source": [
    "## 2 Importing the dataset\n",
    "\n",
    "Data can be retrieved in various formats. The examples below read data from ARFF, JSON and CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9JI_raV6-V5"
   },
   "source": [
    "### Reading from ARFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RrCkWyPS_a2p"
   },
   "outputs": [],
   "source": [
    "# download a copy of an archived data set and extract the zip file to the notebook's folder\n",
    "f_zip = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00212/vertebral_column_data.zip'\n",
    "r = requests.get(f_zip, stream=True)\n",
    "Vertebral_zip = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "Vertebral_zip.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mg8RWWhWBTuc",
    "outputId": "1c1d4f0a-6d53-45d4-c26b-52578738def6"
   },
   "outputs": [],
   "source": [
    "# read the ARFF file and store it as a dataframe\n",
    "data = arff.loadarff('column_2C_weka.arff')\n",
    "df1 = pd.DataFrame(data[0])   #data[1] is the column names\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7sQGGnxCSBi"
   },
   "source": [
    "### Reading from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_4sqh9SrCXvu",
    "outputId": "5f607add-749c-49fd-d44d-bc75ff43636e"
   },
   "outputs": [],
   "source": [
    "# Create a JSON file from excel\n",
    "df2 = pd.read_excel('data2.xlsx',index_col=0) # use column 0 as the row labels\n",
    "df2.to_json('data2.json')\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the newly created JSON as a dataframe\n",
    "df3 = pd.read_json(\"data2.json\")\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCFN-pIs6yb7"
   },
   "source": [
    "### Reading from CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSV file from excel\n",
    "df4 = pd.read_excel('data2.xlsx',index_col=0)\n",
    "df4.to_csv('data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwEPNDWySTKm"
   },
   "outputs": [],
   "source": [
    "# Read CSV files and extract into features and target\n",
    "dataset = pd.read_csv('data2.csv')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhfKXNxlSabC"
   },
   "source": [
    "## 3 Taking care of missing data\n",
    "\n",
    "There are several ways to handle missing data but only the following will be covered in this exercise\n",
    "* remove the rows with missing data.\n",
    "* impute missing values with mean, median or mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping rows with missing data\n",
    "The dropna function's axis argument is default to 0 (along row) where any value within the row being NaN will result in the row being removed. You can set it to one to remove columns with NaN values.\n",
    "\n",
    "Removing missing values creates a strong model but there may be a loss of a lot of data. This will work poorly if the amount of removal is significant in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing values with mean, median or mode\n",
    "\n",
    "With numerical continous values, there is an option to use the mean, median or mode values to fill the missing values. The missing values can also be set to zero or a particular scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with a scalar value\n",
    "#dataset.fillna(0)\n",
    "dataset.replace({np.NaN:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the values into features and target\n",
    "x = dataset.iloc[:, :-1].values\n",
    "y = dataset.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with mean value\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(x[:, 1:3])\n",
    "x[:, 1:3] = imputer.transform(x[:, 1:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "hCsz2yCebe1R",
    "outputId": "1e4cc568-4e51-4b38-9d46-4aa3f15204be"
   },
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CriG6VzVSjcK"
   },
   "source": [
    "## 4 Encoding categorical data\n",
    "\n",
    "Categorical data can only take on a limited and usualy fixed number of values. For example, gender as described by Male or Female, and job positions are categorical.\n",
    "\n",
    "Categorical data can be \n",
    "* Nominal\n",
    "* Ordinal\n",
    "\n",
    "In general, nominal data are labeled with no specific order while ordinal data have a specific order. Gender is a nominal data while the level of satisfaction (indicated as poor/average/good) is ordinal. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AhSpdQWeSsFh"
   },
   "source": [
    "### Encoding the Independent Variable\n",
    "\n",
    "Computer are unable to process categorical data. These data have to be processed and one-hot encoding is widely used because simple labeling using numerical number introduces an order that may not be valid.\n",
    "\n",
    "The basic strategy in One-Hot encoding is to convert each category value into a new column and assign a 1 or 0 (True/False) value to the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in a new dataset from CSV\n",
    "\n",
    "df6 = pd.read_csv('categorical.csv')\n",
    "df6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hwuVddlSwVi"
   },
   "outputs": [],
   "source": [
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "df6 = np.array(ct.fit_transform(df6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "id": "f7QspewyeBfx",
    "outputId": "5b35feef-7fe2-46ef-ce70-80495f94f4ed"
   },
   "outputs": [],
   "source": [
    "print(df6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A sparse matrix is a matrix that is comprised of mostly zero values. Its use can lead to enormous computational savings. The Compressed Sparse Row, also called CSR for short, is often used to represent sparse matrices in machine learning given the efficient access and matrix multiplication that it supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the categorical data of name \n",
    "ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n",
    "x_final = scipy.sparse.csr_matrix(ct.fit_transform(x)).toarray()\n",
    "print(x_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DXh8oVSITIc6"
   },
   "source": [
    "### Encoding the Dependent Variable\n",
    "\n",
    "Label Encoding is used to convert each value in a column to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgHCShVyTOYY"
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "FyhY8-gPpFCa",
    "outputId": "7f76ef29-5423-4c3e-cf69-45fbc366a997"
   },
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qb_vcgm3qZKW"
   },
   "source": [
    "## 5 Splitting the dataset into the Training set and Test set\n",
    "\n",
    "The machine learning alogrithm essentially works in two stage of training and testing but you may see the following definition.\n",
    "\n",
    "Training dataset - The sample of data used to fit the model\n",
    "\n",
    "Validation dataset - The sample of data used to provide an unbiased evaluation of a model fit on the training while tuning model hyperparameters. The evaluation because more biased as skill on the validation dataset is incorporated into the model configuration.\n",
    "\n",
    "Test dataset - The sample of data used to provide an unbiased evaluation of a final model fit on the training dataset\n",
    "\n",
    "The test dataset should be carefully sampled to spaces the various scenarios that a model would encounter in the read world. It would be used once after a model is completely trained while the validation dataset is used as part of the development dataset.\n",
    "\n",
    "For ease of understanding, we will focus on just the training data and test data. For your self-learning, you can search for Cross Validation. In cross validation, you essentially use your training set to generate multiple splits of the Train and Validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pXgA6CzlqbCl"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_final, y, test_size = 0.2, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "GuwQhFdKrYTM",
    "outputId": "de1e527f-c229-4daf-e7c5-ea9d2485148d"
   },
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "TUrX_Tvcrbi4",
    "outputId": "9a041a9b-2642-4828-fa2f-a431d7d77631"
   },
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "pSMHiIsWreQY",
    "outputId": "5afe91e0-9244-4bf5-ec1b-e3e092b85c08"
   },
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "I_tW7H56rgtW",
    "outputId": "2a93f141-2a99-4a69-eec5-c82a3bb8d36b"
   },
   "outputs": [],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpGqbS4TqkIR"
   },
   "source": [
    "## 6 Feature Scaling\n",
    "\n",
    "Feature scaling is a method used to normalize or standardize the range of independent variables or features of data. In data processing, it is also known as data normalization and is generally performed during the data preprocessing step.\n",
    "\n",
    "You will see that feature scaling is carried out after separating the data into training data, and test data. This is to avoid the information from the test data from being used during the scaling of the training data.\n",
    "\n",
    "When data are being used in machine learning, the values of features can have very different ranges. One feature could be in kg while another could be in grams. The value can also be very different in magnitude. For example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Transaction | Volume | Average Price |\n",
    "|---|---|---|\n",
    "|1|50000| 1.45|\n",
    "|2|120000| 2.44|\n",
    "|3|450000| 2.11|\n",
    "|4|700000| 1.60|\n",
    "|5|800000| 1.72|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, with largely huge volume value, it is possible that a machine learning algorithm, which cannot recognize the context of a number '800000' versus '1.72' may put more emphasis and priority on the volume.\n",
    "\n",
    "By scaling the values for each column to a similar range, the perfomance of the a machine learning algorithm can be improved. However, it must be noted that not all machine learning benefit from feature scaling. Distance-based algorithm often benefits from feature scaling while tree-based alogrithms will be insensitive to the scaling of features. Some of these algorithms that benefits include\n",
    "* linear and logistic regression\n",
    "* nearest neighbors\n",
    "* neural networks\n",
    "* support vector machines with radial bias kernel functions\n",
    "* principal components analysis\n",
    "* linear discriminant analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The StandardScaler assumes your data is normally distributed within each feature and will scale them such that the distribution is now centred around 0, with a standard deviation of 1. If data is not normally distributed, this is not the best scaler to use.\n",
    "\n",
    "The MinMaxScaler is the probably the most famous scaling algorithm. It essentially shrinks the range such that the range is now between 0 and 1 (or -1 to 1 if there are negative values). This scaler works better for cases in which the standard scaler might not work so well. If the distribution is not Gaussian or the standard deviation is very small, the min-max scaler works better.\n",
    "\n",
    "There are other scalers such as the RobustScaler, which is similar to Min-Max scaler but as it uses the interquartile range instead of the min-max, it is more robust to outliers. \n",
    "\n",
    "The normalizer normalizes rows (samplewise), and not columns (featurewise). \n",
    "\n",
    "Most business data aims to study relations across samples and to predict for new samples, which will likely benefit from featurewise normalization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AxjSUXFQqo-3"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train[:, 10:] = sc.fit_transform(X_train[:, 10:])\n",
    "X_test[:, 10:] = sc.transform(X_test[:, 10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 154
    },
    "id": "DWPET8ZdlMnu",
    "outputId": "dea86927-5124-4e2a-e974-2804df9a913c"
   },
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "sTXykB_QlRjE",
    "outputId": "b68f0cfc-d07c-48cb-80d0-6800028c41f9"
   },
   "outputs": [],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "Import the dataset from 'data_practice.xlsx' and use the steps you have went through in this practical to prepare the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries \n",
    "(Only need to import libraries/modules once)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "# import numpy, matplotlib.pylot and panda\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "# import arff\n",
    "import requests, io, zipfile\n",
    "from scipy.io import arff\n",
    "\n",
    "# import imputers for handling missing value and encoders\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the excel and remove the unnecessary empty columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take care of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo (make string positions values consistent and drop unnecessary columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read as date time and use only the year\n",
    "x['Joined in (Year)'] = pd.to_datetime(x['Joined in (Year)'], format=\"%Y-%m\", errors =\"coerce\")\n",
    "x['Joined in (Year)'] = x['Joined in (Year)'].dt.year\n",
    "\n",
    "x = x.iloc[:,1:]\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[x['Joined in (Year)'].notna()]\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo Extract the values into features and target\n",
    "\n",
    "\n",
    "\n",
    "#todo (for simplicity, there is not need to use the scipy.sparse.csr_matrix)\n",
    "\n",
    "\n",
    "\n",
    "#todo (encode the target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset for training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing with mean value\n",
    "\n",
    "\n",
    "# Do for both train and test set\n",
    "\n",
    "\n",
    "# Scaling\n",
    "\n",
    "\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "data preparation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
